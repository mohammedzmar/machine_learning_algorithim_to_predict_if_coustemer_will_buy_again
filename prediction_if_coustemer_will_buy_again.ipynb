{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHFlKfGiHIWrLGUX659+sP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedzmar/machine_learning_algorithim_to_predict_if_coustemer_will_buy_again/blob/main/prediction_if_coustemer_will_buy_again.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XVMtpKXYEl9I",
        "outputId": "4e02494f-928a-4e05-d3cf-bb9400c8fe39"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-29b29650-23c8-4230-bf71-e6aa1d3c8514\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-29b29650-23c8-4230-bf71-e6aa1d3c8514\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Audiobooks_data.csv to Audiobooks_data (1).csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_csv_data= np.loadtxt('Audiobooks_data.csv',delimiter = ',')\n",
        "# The inputs are all columns in the csv, except for the first one [:,0]\n",
        "# (which is just the arbitrary customer IDs that bear no useful information),\n",
        "# and the last one [:,-1] (which is our targets)\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
        "\n",
        "# The targets are in the last column. That's how datasets are conventionally organized.\n",
        "targets_all = raw_csv_data[:,-1]\n"
      ],
      "metadata": {
        "id": "Ew32TdHAFs_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balance the dataset"
      ],
      "metadata": {
        "id": "nxgOknnWJPE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many targets are 1 (meaning that the customer did convert)\n",
        "num_one_targets = int(np.sum(targets_all))\n",
        "\n",
        "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
        "zero_targets_counter = 0\n",
        "\n",
        "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
        "# Declare a variable that will do that:\n",
        "indices_to_remove = []\n",
        "\n",
        "# Count the number of targets that are 0. \n",
        "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
        "for i in range(targets_all.shape[0]):\n",
        "    if targets_all[i] == 0:\n",
        "        zero_targets_counter += 1\n",
        "        if zero_targets_counter > num_one_targets:\n",
        "            indices_to_remove.append(i)\n",
        "\n",
        "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
        "# We delete all indices that we marked \"to remove\" in the loop above.\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
      ],
      "metadata": {
        "id": "ZbBxNG03JSUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize the inputs"
      ],
      "metadata": {
        "id": "zwsBABAwJjIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities\n",
        "# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.\n",
        "# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. \n",
        "# The result will be interesting.\n",
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
      ],
      "metadata": {
        "id": "-mqPd-LsJj_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shuffle the data"
      ],
      "metadata": {
        "id": "0o1Swz3LJto3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When the data was collected it was actually arranged by date\n",
        "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
        "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "# Use the shuffled indices to shuffle the inputs and targets.\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ],
      "metadata": {
        "id": "3ghc8PgBJpfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split the dataset into train, validation, and test"
      ],
      "metadata": {
        "id": "t5-p-BZWJx-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of samples\n",
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
        "# Naturally, the numbers are integers.\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "\n",
        "# The 'test' dataset contains all remaining data.\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "# Create variables that record the inputs and targets for training\n",
        "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for validation.\n",
        "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for test.\n",
        "# They are everything that is remaining.\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
        "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
        "# you will get different values, as each time they are shuffled randomly.\n",
        "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
        "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
        "\n",
        "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count,'training data')\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count,'validation data')\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count,'test data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmhRHzuRJ7YX",
        "outputId": "7c3c0082-5e2e-44cf-9336-669b129d0f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1780.0 3579 0.4973456272701872 training data\n",
            "228.0 447 0.5100671140939598 validation data\n",
            "229.0 448 0.5111607142857143 test data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the three datasets in *.npz"
      ],
      "metadata": {
        "id": "9NUXWwxvKHmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the three datasets in *.npz.\n",
        "# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!\n",
        "\n",
        "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "metadata": {
        "id": "bEnC9ntxKI3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "hWNZvmU4N_1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "-BuyNCYJQGy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "\n",
        "# we extract the inputs using the keyword under which we saved them\n",
        "# to ensure that they are all floats, let's also take care of that\n",
        "train_inputs = npz['inputs'].astype(np.float)\n",
        "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
        "train_targets = npz['targets'].astype(np.int)\n",
        "\n",
        "# we load the validation data in the temporary variable\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "# we can load the inputs and the targets in the same line\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
        "\n",
        "# we load the test data in the temporary variable\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "# we create 2 variables that will contain the test inputs and the test targets\n",
        "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
      ],
      "metadata": {
        "id": "GUHTvaFFQLLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "Outline, optimizers, loss, early stopping and training"
      ],
      "metadata": {
        "id": "HfyqgS39QfjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the input and output sizes\n",
        "input_size = 10\n",
        "output_size = 2\n",
        "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
        "hidden_layer_size = 50\n",
        "    \n",
        "# define how the model will look like\n",
        "model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
        "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
        "    # the final layer is no different, we just make sure to activate it with softmax\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
        "])\n",
        "\n",
        "\n",
        "### Choose the optimizer and the loss function\n",
        "\n",
        "# we define the optimizer we'd like to use, \n",
        "# the loss function, \n",
        "# and the metrics we are interested in obtaining at each iteration\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "### Training\n",
        "# That's where we train the model we have built.\n",
        "\n",
        "# set the batch size\n",
        "batch_size = 100\n",
        "\n",
        "# set a maximum number of training epochs\n",
        "max_epochs = 100\n",
        "\n",
        "# set an early stopping mechanism\n",
        "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "# fit the model\n",
        "# note that this time the train, validation and test data are not iterable\n",
        "model.fit(train_inputs, # train inputs\n",
        "          train_targets, # train targets\n",
        "          batch_size=batch_size, # batch size\n",
        "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
        "          # callbacks are functions called by a task when a task is completed\n",
        "          # task here is to check if val_loss is increasing\n",
        "          callbacks=[early_stopping], # early stopping\n",
        "          validation_data=(validation_inputs, validation_targets), # validation data\n",
        "          verbose = 2 # making sure we get enough information about the training process\n",
        "          )  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OmY6XwYQhOC",
        "outputId": "6dd31491-0b2a-4cc0-8af1-425b8c59771e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.6307 - accuracy: 0.6401 - val_loss: 0.5220 - val_accuracy: 0.7248 - 543ms/epoch - 15ms/step\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.4776 - accuracy: 0.7552 - val_loss: 0.4406 - val_accuracy: 0.7942 - 77ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.4242 - accuracy: 0.7748 - val_loss: 0.4059 - val_accuracy: 0.7830 - 87ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3975 - accuracy: 0.7823 - val_loss: 0.3829 - val_accuracy: 0.8098 - 99ms/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3804 - accuracy: 0.7949 - val_loss: 0.3696 - val_accuracy: 0.8233 - 84ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.3694 - accuracy: 0.7958 - val_loss: 0.3579 - val_accuracy: 0.8255 - 91ms/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.3600 - accuracy: 0.8064 - val_loss: 0.3506 - val_accuracy: 0.8233 - 95ms/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.3555 - accuracy: 0.8055 - val_loss: 0.3485 - val_accuracy: 0.8300 - 69ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.3507 - accuracy: 0.8075 - val_loss: 0.3480 - val_accuracy: 0.8255 - 75ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.3474 - accuracy: 0.8053 - val_loss: 0.3389 - val_accuracy: 0.8322 - 95ms/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.3448 - accuracy: 0.8103 - val_loss: 0.3356 - val_accuracy: 0.8255 - 96ms/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.3404 - accuracy: 0.8136 - val_loss: 0.3420 - val_accuracy: 0.8300 - 80ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.3409 - accuracy: 0.8136 - val_loss: 0.3460 - val_accuracy: 0.8188 - 74ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcadbc4de50>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "LeTAP_hkSpsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX0kboM8QfNG",
        "outputId": "8aa75f4d-480c-4fb8-85fe-2d4f2a36e971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8214\n"
          ]
        }
      ]
    }
  ]
}